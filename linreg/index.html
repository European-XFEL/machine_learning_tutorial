
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="..">
      
      
        <link rel="next" href="closed_form/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.9">
    
    
      
        <title>Linear Regression - Machine Learning Tutorial</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.4af4bdda.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
      <link rel="stylesheet" href="https://unpkg.com/katex@0/dist/katex.min.css">
    
      <link rel="stylesheet" href="../stylesheets/pseudo-code.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#linear-regression" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Machine Learning Tutorial" class="md-header__button md-logo" aria-label="Machine Learning Tutorial" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Machine Learning Tutorial
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Linear Regression
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/European-XFEL/machine_learning_tutorial.git" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="./" class="md-tabs__link">
          
  
    
  
  Linear Regression

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../nnets/" class="md-tabs__link">
          
  
    
  
  Neutral Networks

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Machine Learning Tutorial" class="md-nav__button md-logo" aria-label="Machine Learning Tutorial" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Machine Learning Tutorial
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/European-XFEL/machine_learning_tutorial.git" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="./" class="md-nav__link md-nav__link--active">
              
  
  <span class="md-ellipsis">
    Linear Regression
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link md-nav__link--active" for="__nav_2" id="__nav_2_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Linear Regression
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="closed_form/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Closed-form solution
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../nnets/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Neutral Networks
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3" id="__nav_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Neutral Networks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nnets/neural_networks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    The neural network class
    
  </span>
  
    
  
  
    <span class="md-status md-status--new"></span>
  

  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nnets/neural_network_testing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Testing with a simple dataset
    
  </span>
  
    
  
  
    <span class="md-status md-status--new"></span>
  

  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#the-model" class="md-nav__link">
    <span class="md-ellipsis">
      The model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#loss-function" class="md-nav__link">
    <span class="md-ellipsis">
      Loss function
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimization" class="md-nav__link">
    <span class="md-ellipsis">
      Optimization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-gradient-descent-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      The gradient descent algorithm
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#putting-it-all-together" class="md-nav__link">
    <span class="md-ellipsis">
      Putting it all together
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Putting it all together">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#making-a-synthetic-dataset" class="md-nav__link">
    <span class="md-ellipsis">
      Making a synthetic dataset
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fit-the-parameters-of-the-model-and-investigate-your-results" class="md-nav__link">
    <span class="md-ellipsis">
      Fit the parameters of the model and investigate your results
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="linear-regression">Linear Regression</h1>
<p>Our first task, a warm-up if you will, is to build a linear regression model. 
This is an example of a <mark>supervised learning</mark> task where we have a set inputs (features) and a corresponding set of outputs (targets).
To achieve this, we need the three ingredients discussed in the Background section, namely:</p>
<ol>
<li>a model,</li>
<li>a cost (a.k.a. loss) function,</li>
<li>a recipe to optimize the model, i.e., to update its free parameters.</li>
</ol>
<h3 id="the-model">The model</h3>
<p>For now, let's restrict to linear functions of one variable. 
By which we mean a polynomial whose variable, let's call it <span class="arithmatex">\(x\)</span>, does not appear with with powers larger than 1 (so the polynomial does not contain terms with <span class="arithmatex">\(x^2\)</span>, <span class="arithmatex">\(x^3\)</span> etc.). The most general linear function of one variable is thus,
$$
y = f(x) = w\,x + b\,.
$$
This model has 2 <mark>parameters</mark>, the slope <span class="arithmatex">\(w\)</span> and the offset <span class="arithmatex">\(b\)</span>. Our task is to <mark>learn</mark> these parameters in order to best describe a dataset.</p>
<div class="admonition example">
<p class="admonition-title">Implement the model</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span class="k">def</span><span class="w"> </span><span class="nf">fx</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">w</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">y</span> <span class="o">=</span> <span class="o">...</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    <span class="k">return</span> <span class="n">y</span>    
</span></code></pre></div>
</div>
<details class="info">
<summary>Math comment: Definition Polynomials</summary>
<p>For us, a polynomial in a single variable is a function <span class="arithmatex">\(f\)</span> that takes a real number as argument and returns a real number (in short <span class="arithmatex">\(f:\mathbb{R}\rightarrow\mathbb{R}\)</span>) of the from 
$$ f(x) = a_0+a_1 x+a_2 x^2 + \ldots + a_n x^n = \sum_{i=0}^n a_i x^i $$ 
with <span class="arithmatex">\(n\in\mathbb{N}\)</span> and <span class="arithmatex">\(a_i \in \mathbb{R}\)</span> (the symbol "<span class="arithmatex">\(\in\)</span>" means "is element of").</p>
</details>
<details class="info">
<summary>Math comment: Different notions of linearity</summary>
<p>The definition of a <em>linear function</em> as given above describes arbitrary straight lines and is usually formulated in the subfield of <a href="https://en.wikipedia.org/wiki/Mathematical_analysis">analysis</a>. In <a href="https://en.wikipedia.org/wiki/Linear_algebra">linear algebra</a> and the general field of <a href="https://en.wikipedia.org/wiki/Algebra">algebra</a> a different notion of linarity is used. Here a function <span class="arithmatex">\(f:\mathbb{R}\rightarrow \mathbb{R}\)</span> is called linear if and only if<br />
$$ f(\alpha x) = \alpha f(x) \quad \text{and} \quad  f(x+y) = f(x) + f(y) \quad \forall \alpha,x,y\in\mathbb{R}\,, $$
were " $ \forall $ " is the math symbol of "for all".<br />
According to this definition our line from above (<span class="arithmatex">\(f(x) = wx+b\)</span>) is not linear, can you see why?
In algebra one would call such a map <em>affine linear</em> or simply <em>affine</em> but this notion goes beyond our little introduction.</p>
</details>
<details class="question">
<summary>Math side quest (optional!)</summary>
<p>Proof the following statement:<br />
Any algebraically linear function in one variable (see above definition) has the form 
$$ f(x)=ax \quad \text{for a unique } a\in\mathbb{R} $$ 
Hints:</p>
<ol>
<li>You have to use the properties of the difinition of linear functions. </li>
<li>You need to show both existance and uniqueness of such an <span class="arithmatex">\(a\)</span>.</li>
<li>Uniqunes proofs usually go as follows:<ul>
<li>Assume ther exists another <span class="arithmatex">\(a'\)</span> that satisfies the above (i.e. f(x) = x a')</li>
<li>show that then <span class="arithmatex">\(a'=a\)</span> and therfore <span class="arithmatex">\(a\)</span> must be unique.</li>
</ul>
</li>
<li>If you have found a correct proof you can write either "q.e.d." (short for the latin "<em>quod erat demonstrandum</em>" that means "Which was to be proved/Was zu zeigen war") or the more modern " $ \square $ " at the bottom right corner to signal that the proof is finished.</li>
</ol>
</details>
<h3 id="loss-function">Loss function</h3>
<p>The purpose of the loss function is to tell us "how well" we are able model the data for particular values of the parameters. The better the model describes the data, the smaller the loss ought to be.</p>
<p>A common loss function in regression is the Mean-Squared Error (MSE) which is given by
$$
\mathcal{L} = \frac{1}{N}\sum_{i=1}^N \left(y_i - t_i\right)^2 = \frac{1}{N}\sum_{i=1}^N \left(f(x_i) - t_i\right)^2\,,
$$
where <span class="arithmatex">\(N\)</span> is the number of <span class="arithmatex">\((x,t)\)</span>, i.e. (feature, target), pairs in the dataset and <span class="arithmatex">\(y\)</span> is the output of the model. (Technically, this is a measure for the expected value of the loss, <span class="arithmatex">\(\mathbb{E}\left[\mathcal{L}\right]\)</span>.)
This loss function arises naturally if the underlying variation in the data is Gaussian which in many cases is a good assumption. However, one drawback of MSE loss is that it is sensitive to "outliers" in the data. We will examine this in more detail later and discuss possible mitigation strategies.</p>
<div class="admonition example">
<p class="admonition-title">Implement the loss function</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">loss</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">w</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute the MSE loss and return it</span>
</span><span id="__span-1-3"><a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-1-4"><a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>
</span><span id="__span-1-5"><a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a>    <span class="n">mse_loss</span> <span class="o">=</span> <span class="o">...</span>
</span><span id="__span-1-6"><a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>
</span><span id="__span-1-7"><a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a>    <span class="k">return</span> <span class="n">mse_loss</span>
</span></code></pre></div>
</div>
<details class="info">
<summary>Math comment: Expected values, sample mean &amp; probability density</summary>
<p>The notion of expected value belongs to the field of <a href="https://en.wikipedia.org/wiki/Probability_theory">probability theory</a>.<br />
Suppose you conduct an experiment whose outcome is described by a vairable <span class="arithmatex">\(X\)</span> which can only be in one of two states, say <span class="arithmatex">\(x_1\)</span> or <span class="arithmatex">\(x_2\)</span> and let <span class="arithmatex">\(p_1\)</span>, <span class="arithmatex">\(p_2\)</span> be the probabilities with which <span class="arithmatex">\(X\)</span> attains these values (e.g. think of a fair coin toss where we set <span class="arithmatex">\(X=0\)</span> for "tails", <span class="arithmatex">\(X=1\)</span> for "heads" and <span class="arithmatex">\(p_1=p_2=\frac{1}{2}\)</span>), then the <em>expected value</em> <span class="arithmatex">\(\mathbb{E}(X)\)</span> of <span class="arithmatex">\(X\)</span> is given by
$$ \mathbb{E}(X) = x_1 p_1 + x_2 p_2 $$</p>
<p>In such a setting <span class="arithmatex">\(X\)</span> is called a <em>random variable</em>.<br />
By repeatedly measuring outcomes of this experiment we can estimate the true expectation value using the <em>sample mean</em>.
Assume we conducted <span class="arithmatex">\(N\)</span> experiments whose outcomes are given by <span class="arithmatex">\(X_1,\ldots,X_N\)</span> the <em>sample mean</em> is then defined by
$$ \frac{1}{N}\sum_1^N X_i \approx \mathbb{E}(X)$$ 
If you look back at the definition of the loss function from above you can see that it has the form of a sample mean and therfore is a measure for the expected value of the squared difference between the model value at the given features <span class="arithmatex">\(f(x_i)\)</span> and the targests <span class="arithmatex">\(t_i\)</span>.  </p>
<p>The definition of the <em>expectated value</em> given above can be directly generalized to the case where <span class="arithmatex">\(X\)</span> can be in finitely many different states.<br />
Trick question, in how many possible states can our loss function be ? (maybe pause a bit and think about it.)<br />
We are cutting corners here a bit, in order to properly introuce <em>expected values</em> for our case we would need <a href="https://en.wikipedia.org/wiki/Measure_(mathematics)">measures</a>/<a href="https://en.wikipedia.org/wiki/Integral">integrals</a>. These would allow us to assing probabilities to arbitrary collections of loss function values. One way of doing this goes via a <em>probability density functions</em> an example of which can be seen below:
<figure markdown="span">
<img alt="Image title" src="../figures/gaussian.png" width="600" />
<figcaption></figcaption>
</figure>
 On the x-axis are all possible values for the random variable <span class="arithmatex">\(X\)</span>, i.e. all real numbers. The probability with which <span class="arithmatex">\(X\)</span> takes a value in the interval <span class="arithmatex">\([a,b]\)</span> (i.e. the probability for <span class="arithmatex">\(a\leq X \leq b\)</span>) is given by the area that lies under the curve of the <em>probability density</em> in the same interval. In the above example X takes values in [0,1] with a probability of <span class="arithmatex">\(34.1\)</span>%.</p>
</details>
<details class="info">
<summary>Math comment: Gaussians</summary>
<p>A Gaussian is a specific <em>probability distribution</em>, tn fact, the probability density shown in the example above is a Gaussian probability distribution. Gaussian probability distributions are extreamly common in nature and everywhere around you, this is because of the <a href="https://en.wikipedia.org/wiki/Law_of_large_numbers">law of large numbers</a>.<br />
It states that whenever you have many intependent random variables, say <span class="arithmatex">\(X_1, \ldots X_n\)</span>, then the probability distribution 
of their sample mean <span class="arithmatex">\(\frac{1}{n}(X_1 + \ldots + X_n)\)</span> becomes arbitrary close to a Gaussian for increasing <span class="arithmatex">\(n\)</span>.</p>
<p>One consequence of this is that whenever you perform repeated random experiments the sample mean of <span class="arithmatex">\(n\)</span> such experiments, thought of as a random variable itself, can be better and better approximated by a Gaussian probbility distribution the higher <span class="arithmatex">\(n\)</span> becomes. </p>
<p>Example insuracnes:
The more people buy a particular insurance the better the average number of insurance cases per year can be described by a Gaussian probability distribution.</p>
</details>
<h3 id="optimization">Optimization</h3>
<p>Now that we have a model and loss function, we need the third ingredient: a recipe to update the model parameters in order to minimize the loss. Well, the loss function we chose, MSE, is "differentiable" and a method for optimizing differentiable functions has been around since the mid 19th century. This method is known as <em>gradient descent</em> optimization.</p>
<p>Things are getting a bit technical here and there is lot to unpack so let's look at it bit by bit:</p>
<div class="annotate">
<ol>
<li>
<p>The <mark>gradient</mark> of a function of more than one variable is a "vector" (better yet, it <em>transforms</em> as a vector) whose components are the partial derivatives of the function with respect to those variables (1). The derivative of a function at a point is the slope of the tangent to that curve; in the figure just below, you can see a blue curve with red dots, the gray lines are the tangents to the curve at those dots.</p>
</li>
<li>
<p>Now the second part: <mark>descent</mark>. So the derivative tells us the how steep or not the curve is at a point. Well, calculus tells us that the function has an <em>extremum</em>, i.e. a maximum or a minimum, where its (first) derivative is zero! That is, when the tangent curve is flat. <mark>We want to find the minimum because that is where the loss function is smallest</mark>.</p>
</li>
<li>
<p>So the gradient tells us if the function is steep or flat. If it's steep, we want to <strong>descend</strong> down to where it's less steep until we get to the (or <em>a</em>) minimum where it's flat -- that's gradient descent!</p>
</li>
</ol>
</div>
<ol>
<li>Okay, so where did the "more-than-one-variable" thing happen here? Recall that we want to optimize the loss function with respect to the parameters and our model has two of those!</li>
</ol>
<figure>
  <img alt="Image title" src="../figures/tangents.png" width="600" />
  <figcaption>The gray segments are the tangents to the blue curve at each red point. The slope of the tangent lines are the values of the first derivative of the curve at each point.</figcaption>
</figure>
<p>Back to our loss function and its gradient. The <code>loss_gradient</code> function will have to return two values not just one; i.e., the derivatives with respect to the slope, <span class="arithmatex">\(w\)</span>, and the offset, <span class="arithmatex">\(b\)</span>. Those are given by </p>
<div class="arithmatex">\[\begin{equation*}
\begin{split}
\frac{\partial\mathcal{L}}{\partial w} &amp;= \frac{2}{N}\sum_{i=1}^N\left(w\,x_i^2 + b\,x_i - t_i\,x_i\right)\,,\\
\frac{\partial\mathcal{L}}{\partial b} &amp;= \frac{2}{N}\sum_{i=1}^N\left(w\,x_i + b - t_i\right)\,.
\end{split}
\end{equation*}\]</div>
<div class="admonition example">
<p class="admonition-title">Implement the gradient of the loss function</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-2-1"><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">loss_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">w</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
</span><span id="__span-2-2"><a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute the gradien of the loss function</span>
</span><span id="__span-2-3"><a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-2-4"><a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a>
</span><span id="__span-2-5"><a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a>    <span class="n">dloss_dw</span> <span class="o">=</span> <span class="o">...</span>
</span><span id="__span-2-6"><a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a>    <span class="n">dloss_db</span> <span class="o">=</span> <span class="o">...</span>
</span><span id="__span-2-7"><a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a>
</span><span id="__span-2-8"><a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a>    <span class="k">return</span> <span class="p">(</span><span class="n">dloss_dw</span><span class="p">,</span> <span class="n">dloss_db</span><span class="p">)</span>
</span></code></pre></div>
</div>
<h4 id="the-gradient-descent-algorithm">The gradient descent algorithm</h4>
<p>Now that we have the <code>loss_gradient</code> functions implemented, it's time to put it to use as follows:</p>
<div class="annotate">
<ol>
<li>Start with <em>guesses</em> for <span class="arithmatex">\(w\)</span> and <span class="arithmatex">\(b\)</span></li>
<li>Compute the gradient using those values</li>
<li>Update them to the new value <span class="arithmatex">\(w'\)</span> and <span class="arithmatex">\(b'\)</span> as:<ul>
<li><span class="arithmatex">\(w' = w - \eta\,\frac{\partial\mathcal{L}}{\partial w}\)</span></li>
<li><span class="arithmatex">\(b' = b - \eta\,\frac{\partial\mathcal{L}}{\partial b}\)</span></li>
</ul>
</li>
<li>Repeat this procedure until the loss ceases to get smaller (1).</li>
</ol>
<p>The parameter <span class="arithmatex">\(\eta\)</span> should be small (i.e., <span class="arithmatex">\(\eta &lt;1\)</span>) and is called the <mark>learning rate</mark>. Choosing the learning rate is one the most important (and tricky) <mark>hyperparameters</mark> of the model to set.</p>
</div>
<ol>
<li>When and how exactly to stop this procedure is quite a deep topic by itself.</li>
</ol>
<div class="admonition info">
<p class="admonition-title">Stochastic gradient descent</p>
<p>Techinically, computing the gradient as described above means computing the gradient over the entire dataset. This, however, is slow and inefficient. A very convenient way to circumvent these problems is to compute the gradient on a subset of the data (a.k.a. a <strong>batch</strong>). These batches should be chosen randomly and should be anywhere from <span class="arithmatex">\(\sim 32\)</span> points to something larger <img alt="ðŸ˜„" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/1f604.png" title=":smile:" /> depending on the size of the dataset for example.</p>
</div>
<div class="admonition example">
<p class="admonition-title">Implement stochastic gradient descent</p>
<p>The following pseudocode is adapted from the SGD algorithm (8.1) from the <a href="https://www.deeplearningbook.org/">Deep Learning</a> book by Goodfellow, Bengio, and Courville [see chapter 8, page 291]</p>
</div>
<div class='ps-root'>
<div class='ps-algorithm'>
<div class='ps-algorithmic'>
<div class='ps-state ps-indent-0'><span class="arithmatex">\(\text{\textbf{input :}}\)</span> <span class="arithmatex">\(\text{learning rate}\)</span>, <span class="arithmatex">\(\eta\)</span></div>
<div class='ps-state ps-indent-0'><span class="arithmatex">\(\text{\textbf{input :}}\)</span> <span class="arithmatex">\(\text{initial parameters}\)</span>, <span class="arithmatex">\(w \text{ and } b\)</span></div>
<div class='ps-procedure ps-indent-0'><span class='ps-keyword'>procedure </span><span class='ps-funcname'>Stochastic gradient descent</span>(<span class="arithmatex">\(\eta, w, b\)</span>)
<div class='ps-state ps-indent-1'><span class="arithmatex">\(k = 1\)</span></div>
<div class='ps-while ps-indent-1'><span class='ps-keyword'>while</span> (do_another_epoch == True) <span class='ps-keyword'>do</span>
<div class='ps-for ps-indent-2'><span class='ps-keyword'>for</span> (batch in minibatches) <span class='ps-keyword'>do</span>
<div class='ps-state ps-indent-3'>compute loss function</div>
<div class='ps-state ps-indent-3'>compute gradient of loss function: <span class="arithmatex">\(\frac{\partial\mathcal{L}}{\partial w},\frac{\partial\mathcal{L}}{\partial b}\)</span></div>
<div class='ps-state ps-indent-3'>update parameters: <span class="arithmatex">\(w = w - \eta\, \frac{\partial\mathcal{L}}{\partial w}\)</span></div>
<div class='ps-state ps-indent-3'><span class="arithmatex">\(\quad\quad\quad\quad\quad\quad\quad\quad b \,= b - \eta\, \frac{\partial\mathcal{L}}{\partial b}\)</span></div>
<div class='ps-keyword'>end for</div></div>
<div class='ps-state ps-indent-2'>update <span class="arithmatex">\(\mathrm{do\_another\_epoch}\)</span></div>
<div class='ps-state ps-indent-2'><span class="arithmatex">\(k = k+1\)</span></div>
<div class='ps-keyword'>end while</div></div>
</div>
</div>
</div>
</div>

<h2 id="putting-it-all-together">Putting it all together</h2>
<p>Okay so now we have the different pieces that we need to actually <em>do</em> the regression. But first, we need some data. To have full control of the features (i.e., the <em>true</em> underlying model) of this dataset, let's create some synthetic (artificial) data with <code>scikit-learn</code>.</p>
<h3 id="making-a-synthetic-dataset">Making a synthetic dataset</h3>
<p><div class="language-python highlight"><pre><span></span><code><span id="__span-3-1"><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_regression</span>
</span><span id="__span-3-2"><a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a><span class="n">seed</span> <span class="o">=</span> <span class="mi">73022375</span>
</span><span id="__span-3-3"><a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10000</span>
</span><span id="__span-3-4"><a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
</span><span id="__span-3-5"><a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a><span class="n">b_true</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span><span class="o">-</span><span class="mf">0.5</span><span class="p">)</span><span class="o">*</span><span class="mi">2</span>
</span><span id="__span-3-6"><a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a><span class="n">features</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">w_true</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span>
</span><span id="__span-3-7"><a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a>    <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
</span><span id="__span-3-8"><a id="__codelineno-3-8" name="__codelineno-3-8" href="#__codelineno-3-8"></a>    <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span>
</span><span id="__span-3-9"><a id="__codelineno-3-9" name="__codelineno-3-9" href="#__codelineno-3-9"></a>    <span class="n">n_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="__span-3-10"><a id="__codelineno-3-10" name="__codelineno-3-10" href="#__codelineno-3-10"></a>    <span class="n">n_targets</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="__span-3-11"><a id="__codelineno-3-11" name="__codelineno-3-11" href="#__codelineno-3-11"></a>    <span class="n">bias</span><span class="o">=</span><span class="n">b_true</span><span class="p">,</span>
</span><span id="__span-3-12"><a id="__codelineno-3-12" name="__codelineno-3-12" href="#__codelineno-3-12"></a>    <span class="n">noise</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
</span><span id="__span-3-13"><a id="__codelineno-3-13" name="__codelineno-3-13" href="#__codelineno-3-13"></a>    <span class="n">coef</span><span class="o">=</span><span class="kc">True</span>
</span><span id="__span-3-14"><a id="__codelineno-3-14" name="__codelineno-3-14" href="#__codelineno-3-14"></a><span class="p">)</span>
</span><span id="__span-3-15"><a id="__codelineno-3-15" name="__codelineno-3-15" href="#__codelineno-3-15"></a><span class="c1"># NOTE: the X array returned by scipy.datasets.make_regression is not a 1d array  even if n_features=1</span>
</span><span id="__span-3-16"><a id="__codelineno-3-16" name="__codelineno-3-16" href="#__codelineno-3-16"></a><span class="n">features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
</span><span id="__span-3-17"><a id="__codelineno-3-17" name="__codelineno-3-17" href="#__codelineno-3-17"></a><span class="c1">#Note: By default w_true is sampled in the range (0,100) lets rescale that to the range (0,1)</span>
</span><span id="__span-3-18"><a id="__codelineno-3-18" name="__codelineno-3-18" href="#__codelineno-3-18"></a><span class="n">targets</span> <span class="o">=</span> <span class="p">(</span><span class="n">targets</span><span class="o">-</span><span class="n">b_true</span><span class="p">)</span><span class="o">/</span><span class="mi">100</span><span class="o">+</span><span class="n">b_true</span>
</span><span id="__span-3-19"><a id="__codelineno-3-19" name="__codelineno-3-19" href="#__codelineno-3-19"></a><span class="n">w_true</span><span class="o">/=</span><span class="mi">100</span>
</span></code></pre></div>
You can plot the resulting dataset via
<div class="language-python highlight"><pre><span></span><code><span id="__span-4-1"><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</span><span id="__span-4-2"><a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a><span class="c1">#Load a colormap</span>
</span><span id="__span-4-3"><a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a><span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>
</span><span id="__span-4-4"><a id="__codelineno-4-4" name="__codelineno-4-4" href="#__codelineno-4-4"></a><span class="n">cmap</span><span class="o">.</span><span class="n">set_under</span><span class="p">(</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>
</span><span id="__span-4-5"><a id="__codelineno-4-5" name="__codelineno-4-5" href="#__codelineno-4-5"></a>
</span><span id="__span-4-6"><a id="__codelineno-4-6" name="__codelineno-4-6" href="#__codelineno-4-6"></a><span class="c1">#Plot data histogram</span>
</span><span id="__span-4-7"><a id="__codelineno-4-7" name="__codelineno-4-7" href="#__codelineno-4-7"></a><span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
</span><span id="__span-4-8"><a id="__codelineno-4-8" name="__codelineno-4-8" href="#__codelineno-4-8"></a><span class="n">hist</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">hist2d</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">targets</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)),</span><span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span><span class="n">vmin</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-4-9"><a id="__codelineno-4-9" name="__codelineno-4-9" href="#__codelineno-4-9"></a><span class="n">cbar</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">hist</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span><span id="__span-4-10"><a id="__codelineno-4-10" name="__codelineno-4-10" href="#__codelineno-4-10"></a><span class="n">cbar</span><span class="o">.</span><span class="n">set_label</span><span class="p">(</span><span class="s1">&#39;counts&#39;</span><span class="p">)</span>
</span><span id="__span-4-11"><a id="__codelineno-4-11" name="__codelineno-4-11" href="#__codelineno-4-11"></a>
</span><span id="__span-4-12"><a id="__codelineno-4-12" name="__codelineno-4-12" href="#__codelineno-4-12"></a><span class="c1">#Plot true linear model</span>
</span><span id="__span-4-13"><a id="__codelineno-4-13" name="__codelineno-4-13" href="#__codelineno-4-13"></a><span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">features</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">*</span><span class="mf">1.2</span><span class="p">,</span><span class="n">features</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.2</span><span class="p">)</span>
</span><span id="__span-4-14"><a id="__codelineno-4-14" name="__codelineno-4-14" href="#__codelineno-4-14"></a><span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span><span class="n">fx</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span><span class="n">w_true</span><span class="p">,</span><span class="n">b_true</span><span class="p">),</span><span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">)</span>
</span><span id="__span-4-15"><a id="__codelineno-4-15" name="__codelineno-4-15" href="#__codelineno-4-15"></a>
</span><span id="__span-4-16"><a id="__codelineno-4-16" name="__codelineno-4-16" href="#__codelineno-4-16"></a><span class="c1">#Set aspect ration and axis limits and labels</span>
</span><span id="__span-4-17"><a id="__codelineno-4-17" name="__codelineno-4-17" href="#__codelineno-4-17"></a><span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
</span><span id="__span-4-18"><a id="__codelineno-4-18" name="__codelineno-4-18" href="#__codelineno-4-18"></a><span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;feature value&#39;</span><span class="p">)</span>
</span><span id="__span-4-19"><a id="__codelineno-4-19" name="__codelineno-4-19" href="#__codelineno-4-19"></a><span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;target value&#39;</span><span class="p">)</span>
</span><span id="__span-4-20"><a id="__codelineno-4-20" name="__codelineno-4-20" href="#__codelineno-4-20"></a><span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">((</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
</span><span id="__span-4-21"><a id="__codelineno-4-21" name="__codelineno-4-21" href="#__codelineno-4-21"></a><span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">((</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
</span></code></pre></div></p>
<figure>
  <img alt="Image title" src="../figures/dataset.png" width="600" />
  <figcaption>Take a moment and check if you understand what is illustrated here.</figcaption>
</figure>
<h2 id="fit-the-parameters-of-the-model-and-investigate-your-results">Fit the parameters of the model and investigate your results</h2>
<p>Using SGD as described <a href="#the-gradient-descent-algorithm">above</a>, fit the parameters of the model and compare them with the parameters used to generate the dataset. </p>
<p>Here are a few things you should carefully consider (and <strong>experiment</strong> with!):</p>
<div class="annotate">
<ul>
<li>The initial guesses for <span class="arithmatex">\(w\)</span> and <span class="arithmatex">\(b\)</span>.</li>
<li>The learning rate: a good starting value is something like 0.01 or 0.001 but play around and see what different values do.</li>
<li>The number of training epochs. (1) Did you ask for too many or too few? How can you tell (see list below for hints).</li>
</ul>
</div>
<ol>
<li>Epochs count how many times you use the entire dataset during training, i.e., how many times the outer loop of the SGD algorithm described above is executed.</li>
</ol>
<p>And here are a few things you should <em>definitely</em> do:</p>
<ul>
<li>Plot the loss (at least <code class="language-text highlight">plt.semilogy</code> or <code class="language-text highlight">plt.loglog</code>) as a function of training steps (batches).<ul>
<li>Is it smooth or does have a lot of noise?</li>
<li>Are there features like a "knee" where the behavior qualitatively changes?</li>
<li>Did the loss reach an asymptotic value? If so, did the training continue for long after that?</li>
</ul>
</li>
<li>Plot the model parameters as function of training steps</li>
<li>Plot the data (<code class="language-python highlight"><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span></code>) and the fitted model</li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href=".." class="md-footer__link md-footer__link--prev" aria-label="Previous: Home">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Home
              </div>
            </div>
          </a>
        
        
          
          <a href="closed_form/" class="md-footer__link md-footer__link--next" aria-label="Next: Closed-form solution">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Closed-form solution
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["content.code.select", "content.code.copy", "navigation.indexes", "navigation.tabs", "navigation.tabs.sticky", "navigation.footer"], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.c8b220af.min.js"></script>
      
        <script src="../javascripts/katex.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/katex.min.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/contrib/auto-render.min.js"></script>
      
    
  </body>
</html>