{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Background","text":""},{"location":"#what-is-machine-learning","title":"What is machine learning?","text":"<p>Here is a nice introductory article from IBM, one from Quanta magazine, one from UC Berkeley which is where the following (highly paraphrased) three ingredients were (to my knowledge) first outlined:</p> <ol> <li>A model that takes data and \"guesses\" a pattern.</li> <li>An error function that tells you how well the model guessed the pattern underlying the data.</li> <li>A way to update the model parameters based on how good or bad the guess is.</li> </ol> <p>In this guided tour, we will mostly focus on supervised learning tasks with increasing complexity. The goal is to mostly implement things by hand to try to understand how everything works together to produce a useful model.</p>"},{"location":"linreg/","title":"Linear Regression","text":"<p>Our first task, a warm-up if you will, is to build a linear regression model.  This is an example of a supervised learning task where we have a set inputs (features) and a corresponding set of outputs (targets). To achieve this, we need the three ingredients discussed in the Background section, namely:</p> <ol> <li>a model,</li> <li>a cost (a.k.a. loss) function,</li> <li>a recipe to optimize the model, i.e., to update its free parameters.</li> </ol>"},{"location":"linreg/#the-model","title":"The model","text":"<p>For now, let's restrict to linear functions of one variable.  By which we mean a polynomial whose variable, let's call it \\(x\\), does not appear with with powers larger than 1 (so the polynomial does not contain terms with \\(x^2\\), \\(x^3\\) etc.). The most general linear function of one variable is thus, $$ y = f(x) = w\\,x + b\\,. $$ This model has 2 parameters, the slope \\(w\\) and the offset \\(b\\). Our task is to learn these parameters in order to best describe a dataset.</p> <p>Implement the model</p> <pre><code>import numpy as np\n\ndef fx(x: np.ndarray, w: float, b: float) -&gt; np.ndarray:\n\n    y = ...\n\n    return y    \n</code></pre> Math comment: Definition Polynomials <p>For us, a polynomial in a single variable is a function \\(f\\) that takes a real number as argument and returns a real number (in short \\(f:\\mathbb{R}\\rightarrow\\mathbb{R}\\)) of the from  $$ f(x) = a_0+a_1 x+a_2 x^2 + \\ldots + a_n x^n = \\sum_{i=0}^n a_i x^i $$  with \\(n\\in\\mathbb{N}\\) and \\(a_i \\in \\mathbb{R}\\) (the symbol \"\\(\\in\\)\" means \"is element of\").</p> Math comment: Different notions of linearity <p>The definition of a linear function as given above describes arbitrary straight lines and is usually formulated in the subfield of analysis. In linear algebra and the general field of algebra a different notion of linarity is used. Here a function \\(f:\\mathbb{R}\\rightarrow \\mathbb{R}\\) is called linear if and only if $$ f(\\alpha x) = \\alpha f(x) \\quad \\text{and} \\quad  f(x+y) = f(x) + f(y) \\quad \\forall \\alpha,x,y\\in\\mathbb{R}\\,, $$ were \" $ \\forall $ \" is the math symbol of \"for all\". According to this definition our line from above (\\(f(x) = wx+b\\)) is not linear, can you see why? In algebra one would call such a map affine linear or simply affine but this notion goes beyond our little introduction.</p> Math side quest (optional!) <p>Proof the following statement: Any algebraically linear function in one variable (see above definition) has the form  $$ f(x)=ax \\quad \\text{for a unique } a\\in\\mathbb{R} $$  Hints:</p> <ol> <li>You have to use the properties of the difinition of linear functions. </li> <li>You need to show both existance and uniqueness of such an \\(a\\).</li> <li>Uniqunes proofs usually go as follows:<ul> <li>Assume ther exists another \\(a'\\) that satisfies the above (i.e. f(x) = x a')</li> <li>show that then \\(a'=a\\) and therfore \\(a\\) must be unique.</li> </ul> </li> <li>If you have found a correct proof you can write either \"q.e.d.\" (short for the latin \"quod erat demonstrandum\" that means \"Which was to be proved/Was zu zeigen war\") or the more modern \" $ \\square $ \" at the bottom right corner to signal that the proof is finished.</li> </ol>"},{"location":"linreg/#loss-function","title":"Loss function","text":"<p>The purpose of the loss function is to tell us \"how well\" we are able model the data for particular values of the parameters. The better the model describes the data, the smaller the loss ought to be.</p> <p>A common loss function in regression is the Mean-Squared Error (MSE) which is given by $$ \\mathcal{L} = \\frac{1}{N}\\sum_{i=1}^N \\left(y_i - t_i\\right)^2 = \\frac{1}{N}\\sum_{i=1}^N \\left(f(x_i) - t_i\\right)^2\\,, $$ where \\(N\\) is the number of \\((x,t)\\), i.e. (feature, target), pairs in the dataset and \\(y\\) is the output of the model. (Technically, this is a measure for the expected value of the loss, \\(\\mathbb{E}\\left[\\mathcal{L}\\right]\\).) This loss function arises naturally if the underlying variation in the data is Gaussian which in many cases is a good assumption. However, one drawback of MSE loss is that it is sensitive to \"outliers\" in the data. We will examine this in more detail later and discuss possible mitigation strategies.</p> <p>Implement the loss function</p> <pre><code>def loss(x: np.ndarray, t: np.ndarray, w: float, b: float) -&gt; float:\n    \"\"\"Compute the MSE loss and return it\n    \"\"\"\n\n    mse_loss = ...\n\n    return mse_loss\n</code></pre> Math comment: Expected values, sample mean &amp; probability density <p>The notion of expected value belongs to the field of probability theory. Suppose you conduct an experiment whose outcome is described by a vairable \\(X\\) which can only be in one of two states, say \\(x_1\\) or \\(x_2\\) and let \\(p_1\\), \\(p_2\\) be the probabilities with which \\(X\\) attains these values (e.g. think of a fair coin toss where we set \\(X=0\\) for \"tails\", \\(X=1\\) for \"heads\" and \\(p_1=p_2=\\frac{1}{2}\\)), then the expected value \\(\\mathbb{E}(X)\\) of \\(X\\) is given by $$ \\mathbb{E}(X) = x_1 p_1 + x_2 p_2 $$</p> <p>In such a setting \\(X\\) is called a random variable. By repeatedly measuring outcomes of this experiment we can estimate the true expectation value using the sample mean. Assume we conducted \\(N\\) experiments whose outcomes are given by \\(X_1,\\ldots,X_N\\) the sample mean is then defined by $$ \\frac{1}{N}\\sum_1^N X_i \\approx \\mathbb{E}(X)$$  If you look back at the definition of the loss function from above you can see that it has the form of a sample mean and therfore is a measure for the expected value of the squared difference between the model value at the given features \\(f(x_i)\\) and the targests \\(t_i\\).  </p> <p>The definition of the expectated value given above can be directly generalized to the case where \\(X\\) can be in finitely many different states. Trick question, in how many possible states can our loss function be ? (maybe pause a bit and think about it.) We are cutting corners here a bit, in order to properly introuce expected values for our case we would need measures/integrals. These would allow us to assing probabilities to arbitrary collections of loss function values. One way of doing this goes via a probability density functions an example of which can be seen below:    On the x-axis are all possible values for the random variable \\(X\\), i.e. all real numbers. The probability with which \\(X\\) takes a value in the interval \\([a,b]\\) (i.e. the probability for \\(a\\leq X \\leq b\\)) is given by the area that lies under the curve of the probability density in the same interval. In the above example X takes values in [0,1] with a probability of \\(34.1\\)%.</p> Math comment: Gaussians <p>A Gaussian is a specific probability distribution, tn fact, the probability density shown in the example above is a Gaussian probability distribution. Gaussian probability distributions are extreamly common in nature and everywhere around you, this is because of the law of large numbers. It states that whenever you have many intependent random variables, say \\(X_1, \\ldots X_n\\), then the probability distribution  of their sample mean \\(\\frac{1}{n}(X_1 + \\ldots + X_n)\\) becomes arbitrary close to a Gaussian for increasing \\(n\\).</p> <p>One consequence of this is that whenever you perform repeated random experiments the sample mean of \\(n\\) such experiments, thought of as a random variable itself, can be better and better approximated by a Gaussian probbility distribution the higher \\(n\\) becomes. </p> <p>Example insuracnes: The more people buy a particular insurance the better the average number of insurance cases per year can be described by a Gaussian probability distribution.</p>"},{"location":"linreg/#optimization","title":"Optimization","text":"<p>Now that we have a model and loss function, we need the third ingredient: a recipe to update the model parameters in order to minimize the loss. Well, the loss function we chose, MSE, is \"differentiable\" and a method for optimizing differentiable functions has been around since the mid 19th century. This method is known as gradient descent optimization.</p> <p>Things are getting a bit technical here and there is lot to unpack so let's look at it bit by bit:</p> <ol> <li> <p>The gradient of a function of more than one variable is a \"vector\" (better yet, it transforms as a vector) whose components are the partial derivatives of the function with respect to those variables (1). The derivative of a function at a point is the slope of the tangent to that curve; in the figure just below, you can see a blue curve with red dots, the gray lines are the tangents to the curve at those dots.</p> </li> <li> <p>Now the second part: descent. So the derivative tells us the how steep or not the curve is at a point. Well, calculus tells us that the function has an extremum, i.e. a maximum or a minimum, where its (first) derivative is zero! That is, when the tangent curve is flat. We want to find the minimum because that is where the loss function is smallest.</p> </li> <li> <p>So the gradient tells us if the function is steep or flat. If it's steep, we want to descend down to where it's less steep until we get to the (or a) minimum where it's flat -- that's gradient descent!</p> </li> </ol> <ol> <li>Okay, so where did the \"more-than-one-variable\" thing happen here? Recall that we want to optimize the loss function with respect to the parameters and our model has two of those!</li> </ol> The gray segments are the tangents to the blue curve at each red point. The slope of the tangent lines are the values of the first derivative of the curve at each point. <p>Back to our loss function and its gradient. The <code>loss_gradient</code> function will have to return two values not just one; i.e., the derivatives with respect to the slope, \\(w\\), and the offset, \\(b\\). Those are given by </p> \\[\\begin{equation*} \\begin{split} \\frac{\\partial\\mathcal{L}}{\\partial w} &amp;= \\frac{2}{N}\\sum_{i=1}^N\\left(w\\,x_i^2 + b\\,x_i - t_i\\,x_i\\right)\\,,\\\\ \\frac{\\partial\\mathcal{L}}{\\partial b} &amp;= \\frac{2}{N}\\sum_{i=1}^N\\left(w\\,x_i + b - t_i\\right)\\,. \\end{split} \\end{equation*}\\] <p>Implement the gradient of the loss function</p> <pre><code>def loss_gradient(x: np.ndarray, t: np.ndarray, w: float, b: float) -&gt; tuple[float, float]:\n    \"\"\"Compute the gradien of the loss function\n    \"\"\"\n\n    dloss_dw = ...\n    dloss_db = ...\n\n    return (dloss_dw, dloss_db)\n</code></pre>"},{"location":"linreg/#the-gradient-descent-algorithm","title":"The gradient descent algorithm","text":"<p>Now that we have the <code>loss_gradient</code> functions implemented, it's time to put it to use as follows:</p> <ol> <li>Start with guesses for \\(w\\) and \\(b\\)</li> <li>Compute the gradient using those values</li> <li>Update them to the new value \\(w'\\) and \\(b'\\) as:<ul> <li>\\(w' = w - \\eta\\,\\frac{\\partial\\mathcal{L}}{\\partial w}\\)</li> <li>\\(b' = b - \\eta\\,\\frac{\\partial\\mathcal{L}}{\\partial b}\\)</li> </ul> </li> <li>Repeat this procedure until the loss ceases to get smaller (1).</li> </ol> <p>The parameter \\(\\eta\\) should be small (i.e., \\(\\eta &lt;1\\)) and is called the learning rate. Choosing the learning rate is one the most important (and tricky) hyperparameters of the model to set.</p> <ol> <li>When and how exactly to stop this procedure is quite a deep topic by itself.</li> </ol> <p>Stochastic gradient descent</p> <p>Techinically, computing the gradient as described above means computing the gradient over the entire dataset. This, however, is slow and inefficient. A very convenient way to circumvent these problems is to compute the gradient on a subset of the data (a.k.a. a batch). These batches should be chosen randomly and should be anywhere from \\(\\sim 32\\) points to something larger  depending on the size of the dataset for example.</p> <p>Implement stochastic gradient descent</p> <p>The following pseudocode is adapted from the SGD algorithm (8.1) from the Deep Learning book by Goodfellow, Bengio, and Courville [see chapter 8, page 291]</p> \\(\\text{\\textbf{input :}}\\) \\(\\text{learning rate}\\), \\(\\eta\\) \\(\\text{\\textbf{input :}}\\) \\(\\text{initial parameters}\\), \\(w \\text{ and } b\\) procedure Stochastic gradient descent(\\(\\eta, w, b\\)) \\(k = 1\\) while (do_another_epoch == True) do for (batch in minibatches) do compute loss function compute gradient of loss function: \\(\\frac{\\partial\\mathcal{L}}{\\partial w},\\frac{\\partial\\mathcal{L}}{\\partial b}\\) update parameters: \\(w = w - \\eta\\, \\frac{\\partial\\mathcal{L}}{\\partial w}\\) \\(\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad b \\,= b - \\eta\\, \\frac{\\partial\\mathcal{L}}{\\partial b}\\) end for update \\(\\mathrm{do\\_another\\_epoch}\\) \\(k = k+1\\) end while"},{"location":"linreg/#putting-it-all-together","title":"Putting it all together","text":"<p>Okay so now we have the different pieces that we need to actually do the regression. But first, we need some data. To have full control of the features (i.e., the true underlying model) of this dataset, let's create some synthetic (artificial) data with <code>scikit-learn</code>.</p>"},{"location":"linreg/#making-a-synthetic-dataset","title":"Making a synthetic dataset","text":"<p><pre><code>from sklearn.datasets import make_regression\nseed = 73022375\nn_samples = 10000\nnp.random.seed(seed)\nb_true = (np.random.rand()-0.5)*2\nfeatures, targets, w_true = make_regression(\n    random_state=seed,\n    n_samples=n_samples,\n    n_features=1,\n    n_targets=1,\n    bias=b_true,\n    noise=20,\n    coef=True\n)\n# NOTE: the X array returned by scipy.datasets.make_regression is not a 1d array  even if n_features=1\nfeatures = np.squeeze(features)\n#Note: By default w_true is sampled in the range (0,100) lets rescale that to the range (0,1)\ntargets = (targets-b_true)/100+b_true\nw_true/=100\n</code></pre> You can plot the resulting dataset via <pre><code>from matplotlib import pyplot as plt\n#Load a colormap\ncmap = plt.get_cmap('viridis')\ncmap.set_under('white')\n\n#Plot data histogram\nfig,ax = plt.subplots()\nhist = ax.hist2d(features,targets,bins=int(np.sqrt(n_samples)),cmap=cmap,vmin=1)\ncbar = fig.colorbar(hist[-1])\ncbar.set_label('counts')\n\n#Plot true linear model\nxs = np.linspace(features.min()*1.2,features.max()*1.2)\nax.plot(xs,fx(xs,w_true,b_true),color = 'red')\n\n#Set aspect ration and axis limits and labels\nax.set_aspect('equal')\nax.set_xlabel('feature value')\nax.set_ylabel('target value')\nax.set_xlim((-4,4))\nax.set_ylim((-4,4))\n</code></pre></p> Take a moment and check if you understand what is illustrated here."},{"location":"linreg/#fit-the-parameters-of-the-model-and-investigate-your-results","title":"Fit the parameters of the model and investigate your results","text":"<p>Using SGD as described above, fit the parameters of the model and compare them with the parameters used to generate the dataset. </p> <p>Here are a few things you should carefully consider (and experiment with!):</p> <ul> <li>The initial guesses for \\(w\\) and \\(b\\).</li> <li>The learning rate: a good starting value is something like 0.01 or 0.001 but play around and see what different values do.</li> <li>The number of training epochs. (1) Did you ask for too many or too few? How can you tell (see list below for hints).</li> </ul> <ol> <li>Epochs count how many times you use the entire dataset during training, i.e., how many times the outer loop of the SGD algorithm described above is executed.</li> </ol> <p>And here are a few things you should definitely do:</p> <ul> <li>Plot the loss (at least <code>plt.semilogy</code> or <code>plt.loglog</code>) as a function of training steps (batches).<ul> <li>Is it smooth or does have a lot of noise?</li> <li>Are there features like a \"knee\" where the behavior qualitatively changes?</li> <li>Did the loss reach an asymptotic value? If so, did the training continue for long after that?</li> </ul> </li> <li>Plot the model parameters as function of training steps</li> <li>Plot the data (<code>plt.scatter</code>) and the fitted model</li> </ul>"},{"location":"linreg/closed_form/","title":"Closed-form solution","text":"<p>First, let us generalize the loss function just a little bit more by adding a regularization term to it -- see this Wikipedia article for a very brief introduction to the topic.</p> Why would we want to regularize the model? <p>In this simple case with a one-dimensional function (i.e., with one dependent variable), regularization does not actually help; this is easy to show explicitly . However, when the model becomes more complex and has many parameters, the regularization term penalizes complexity and, thus, overfitting.</p> <p>With this additional term, the loss function becomes, $$ \\mathcal{L} = \\frac{1}{N}\\sum_{i=1}^N \\left(y_i - t_i\\right)^2 + \\lambda\\,\\Omega(w,b)\\,, $$ where \\(\\lambda\\) is a coefficient that controls the relative size (i.e., effect) of the regularization term, \\(\\Omega\\) which is a function of the parameters, \\(w\\) and \\(b\\). Two well established (and well motivated) choices for the regularization term are:</p> Type Form of \\(\\mathbf{\\Omega}\\) Used in \\(L_2-\\)norm \\(w^2 + b^2\\) Ridge regression \\(L_1-\\)norm \\(\\lvert w\\rvert + \\lvert b\\rvert\\) LASSO <p>In this slightly more complicated but still simple case, and with a bit of algebra, we can find a closed form solution for the parameters \\(w\\) and \\(b\\). The loss function here is strictly convex and thus has a unique minimum. The solution can be found by solving the linear set of equations \\(\\nabla\\mathcal{L}=\\vec{0}\\) with \\(L_2\\) regularization and is given by,</p> \\[\\begin{equation*} \\begin{split} w &amp;= \\frac{\\mathrm{cov}(X, T) + \\lambda\\langle XT\\rangle}{ \\mathrm{var}\\,X + \\lambda\\langle X^2\\rangle + \\lambda(1+\\lambda) }\\,,\\\\ b &amp;= \\frac{1}{1+\\lambda}\\left[\\langle{T}\\rangle - w\\,\\langle{X}\\rangle\\right]\\,. \\end{split} \\end{equation*}\\] <p>The captial letters mean the full vector of features, \\(x\\), and targets, \\(t\\), in the dataset. The operators \\(\\mathrm{cov}\\) and \\(\\mathrm{var}\\) are the covariance and variance respectively. They can be computed with the NumPy functions <code>numpy.cov</code> and <code>numpy.var</code>. Finally, quantities enclosed in angled brackets as in \\(\\langle{X}\\rangle\\) means the average (<code>numpy.mean</code>) over the features in the datasets and similarly for the targets and the product of the features with the targets.</p>"},{"location":"linreg/closed_form/#estimating-the-error-on-the-fitted-parameters","title":"Estimating the error on the fitted parameters","text":"<p>This section is techincal</p> <p>One can get a lower bound on the variance of the fitted parameters (or, to be more technically correct, the variance of the estimators). This is known as the Cram\u00e9r-Rao bound which says this variance is bounded from below by the inverse of the Fisher information, i.e.,</p> \\[\\begin{equation*} \\mathrm{var}(\\hat{\\boldsymbol\\theta})\\geq I(\\boldsymbol\\theta)^{-1}\\,, \\end{equation*}\\] <p>where \\(I(\\boldsymbol\\theta)\\) is the Fisher information matrix which is proportional to the Hessian of the loss function</p> \\[\\begin{equation*} \\left[I(\\boldsymbol\\theta)\\right]_{ij} = -N\\,\\mathbb{E}\\left[     \\frac{\\partial^2}{\\partial\\theta_i\\partial\\theta_j} \\log\\,f(\\boldsymbol{X};\\boldsymbol\\theta)\\Biggr|\\boldsymbol\\theta \\right]\\,, \\end{equation*}\\] <p>where \\(f\\) is the likelihood function (1) which is related to our loss function via</p> <ol> <li>I'm slightly confused here , this (taken from Wikipedia) is not a likelihood but looks rather like a probability...</li> </ol> \\[\\begin{equation*} -\\log f = \\frac{\\mathcal{L}}{2S^2}\\,. \\end{equation*}\\] <p>Note that the (unbiased) sample variance, \\(S^2\\), appears here if we assume a Gaussian likelihood. So, we can write \\(I_{ij}\\) as</p> \\[\\begin{equation*} \\left[I(\\boldsymbol\\theta)\\right]_{ij} = \\frac{1}{2S^2}\\,\\frac{\\partial^2\\mathcal{L}}{\\partial\\theta_i\\partial\\theta_j} = \\frac{1}{2S^2}\\,H_{ij}\\,, \\end{equation*}\\] <p>where \\(H_{ij}\\) is a matrix element of the Hessian matrix. Thus, the inverse of the Fisher information matrix is (dropping the explicit dependence on the arguments for simplicity)</p> \\[\\begin{equation*} I^{-1} = 2S^2\\,H^{-1} = \\frac{2}{N-1}\\sum_i^N(y_i-t_i)^2\\,H^{-1} \\end{equation*}\\] <p>Denoting the estimators for \\(w\\) and \\(b\\) by \\(\\hat{w}\\) and \\(\\hat{b}\\), their (co)variances are thus,</p> <p>(Co)variances of the estimators</p> \\[\\begin{equation*} \\begin{split} \\mathrm{var}\\,\\hat{w} &amp;= \\frac{1}{N-1}\\sum_i^N (y_i-t_i)^2\\times   \\frac{1}{\\mathrm{var}\\,X}\\left(\\mathrm{var}\\,X + \\overline{X}^2\\right)\\,,\\\\ \\mathrm{var}\\,\\hat{b} &amp;= \\frac{1}{N-1}\\sum_i^N (y_i-t_i)^2\\times    \\frac{1}{\\mathrm{var}\\,X}\\,,\\\\ \\mathrm{cov}(\\hat{w}, \\hat{b}) &amp;= \\frac{-1}{N-1}\\sum_i^N (y_i-t_i)^2\\times    \\frac{\\overline{X}}{\\mathrm{var}\\,X}\\,. \\end{split} \\end{equation*}\\]"},{"location":"linreg/closed_form/#propagating-the-errors-to-the-model","title":"Propagating the errors to the model","text":"<p>Let's keep using the hatted parameters (estimated from the data) to distinguish them from the true parameters. Our model is then $$ y = \\hat{w}x + \\hat{b}. $$</p> <p>Propagating the uncertainties to \\(y\\) (see this Wikipedia article for example), we have $$ (\\delta y)^2 = (\\mathrm{var}\\,\\hat{w})\\,x^2 + \\mathrm{var}\\,\\hat{b} + 2x\\,\\mathrm{cov}(\\hat{w}, \\hat{b})\\,. $$</p>    The band shows the 95% confidence interval on the fit by propagating the uncertainty on the estimated parameters $\\hat{w}$ and $\\hat{b}$ as described in the main text."},{"location":"nnets/","title":"Neural networks","text":"<p>In this tutorial, we are going to implement a fully-connected feed-forward neural network from scratch. We will then use this neural network to classify the handwritten digits in MNIST dataset [LeCun, Cortes, and Burges].</p> <p>The following resources provide a nice basic introduction to neural networks:</p> <ul> <li>https://www.ibm.com/topics/neural-networks</li> <li>https://en.wikipedia.org/wiki/Neural_network_(machine_learning)</li> <li>https://realpython.com/python-ai-neural-network/</li> <li>You can also read Chris Bishop's new book online, see https://www.bishopbook.com/. Chapter 4 discusses single layer neural networks.</li> <li>And his classic PRML book can also be downloaded from Microsoft for free here https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/. Chapter 5 dicusses neural networks.</li> </ul> <p>We'll start by coding the neural network. Once done, we'll turn our attention to the MNIST dataset, in particular, we'll try to get familiar with it, plot a few of the digits, figure out how to transform the images into an input for our neural network, etc.</p>"},{"location":"nnets/neural_network_testing/","title":"A simple dataset for testing","text":"<p>The (U.S.) National Institute of Standards and Technology (NIST) has a nice collection of datasets for non-linear regression along with 'certified' fit parameters.</p> <p>The page that lists these datasets is itl.nist.gov/div898/strd/nls/nls_main.shtml. You can choose whichever model you like. But to be concrete here, I will take the <code>Chwirut2</code> dataset which is exponential-distributed and described by a 3-parameter model.</p> <p>Let's also implement the model using the NIST certified parameters. The model is $$ f(x; \\beta) + \\epsilon = \\frac{\\exp(-\\beta_1 x)}{\\beta_2 + \\beta_3 x} + \\epsilon\\,, $$ with \\(\\beta_1\\), \\(\\beta_2\\), and \\(\\beta_3\\) given by <pre><code>               Certified              Certified\nParameter      Estimate               Std. Dev. of Est.\n  beta(1)      1.6657666537E-01       3.8303286810E-02\n  beta(2)      5.1653291286E-03       6.6621605126E-04\n  beta(3)      1.2150007096E-02       1.5304234767E-03\n</code></pre></p> <p>Here is the Python implementation of this model with the best-fit parameters. <pre><code>def fcert(x: np.ndarray) -&gt; np.ndarray:\n\n    beta1 = 1.6657666537E-01\n    beta2 = 5.1653291286E-03\n    beta3 = 1.2150007096E-02\n\n    return np.exp(-beta1*x) / (beta2 + beta3*x)\n</code></pre></p> <p>Normalizing the dataset</p> <p>For many reasons, neural networks do care about the normalization of the data. In particular, when using the <code>sigmoid</code> activation function wich has a range \\(\\in [0,1]\\), this would save a lot of unecessary frustration.</p> Network prediction during training."},{"location":"nnets/neural_networks/","title":"The <code>Network</code> <code>class</code>","text":"<p>To efficiently and cleanly write the necessary code, it is very useful to create a <code>class</code>. To read about classes in Python see realpython.com/python-classes (reading the Getting Started With Python Classes section will at least get you familiar with the basic concepts). The class definition and the initialization code (<code>__init__</code> method) as well as the definition of the required methods are given just below. Your task is to understand the relevant algorithms and fill in the corresponding methods. Let's go!</p> <p>Beware</p> <p>The <code>itertools</code> module is part of the Python standard library. However, <code>pairwise</code> was only added in Python v3.10. Make sure this version requirement is satisfied otherwise see footnote for alternative.</p> <pre><code>class Network:\n    def __init__(self, definition: tuple):\n        self.definition = definition\n        self.loss = 0\n        self.layer_ids = range(len(definition))\n        self.activations = {}\n        self.layers = {}\n        for idx in self.layer_ids:\n            self.layers[idx] = np.zeros(self.definition[idx])\n            self.activations[idx] = np.zeros_like(self.layers[idx])\n        # Initialize the weights and biases as dictionaries of np.ndarray\n        self.weights = {}\n        self.biases = {}\n        self.weights_grad = {}\n        self.biases_grad = {}\n        for idx, (ii, jj) in enumerate(pairwise(definition)):\n            self.weights[idx+1] = np.random.rand(ii, jj)\n            self.biases[idx+1] = np.zeros(jj)\n            self.weights_grad[idx+1] = np.random.rand(ii, jj)\n            self.biases_grad[idx+1] = np.zeros(jj)\n\n        # Initialize the input and output vectors (later can generalize to\n        # tensors if necessary)\n        self.input = np.zeros(definition[0])\n        self.output = np.zeros(definition[-1])\n\n\n    def forward(self, features: np.ndarray, targets: np.ndarray):\n        \"\"\"Implements the forward propagation algorithm\n        \"\"\"\n        assert self.output.shape == targets.shape\n        ...\n        self.loss = np.sum(np.square(self.output - targets))\n        return None\n\n\n    def backprop(self, targets: np.ndarray):\n        \"\"\"Implements the backpropagation algorithm\n        \"\"\" \n        assert self.output.shape == targets.shape\n        grad = (targets - self.output)\n        ...\n        return None\n\n\n    def compute_loss\n</code></pre>"},{"location":"nnets/neural_networks/#the-forward-pass","title":"The forward pass","text":"<p>Propagating the input through the neural network and computing the resulting output is known as forward propagation. This algorithm that implements it is described below and closely follows Algorithm 6.3 in Deep Learning (chapter 6, page 208) by Goodfellow, Bengio, and Courville <code>[Goodfellow-et-al-2016]</code>.</p> <p>Note</p> <p>The weight matrices are defined differently here than in the book. Specifically, the definition here is related to the one in the book by matrix transposition.</p> <pre>\nalgorithm: forward propagation through a fully connected neural network\ninput: W<sup>(i)</sup>, i\u2208{1,...,l}; the weights of the model\n       b<sup>(i)</sup>, i\u2208{1,...,l}; the biases of the model\n       x, the input (features)\n       t, the targets\nrequire: f, activation function\n\nh<sup>(0)</sup> = x\nfor k=1,...,l do\n   a<sup>(k)</sup> = b<sup>(k)</sup> + h<sup>(k-1)</sup>W<sup>(k)</sup>\n   h<sup>(k)</sup> = f(a<sup>(k)</sup>)\nend for\ny = h<sup>(l)</sup>\nJ = loss(y, t)\n</pre>"},{"location":"nnets/neural_networks/#backpropagation","title":"Backpropagation","text":"<p>What is backpropagation? See this article from IBM, for example. The Wikipedia article is quite detailed but also very technical.</p> <p>The pseudocode below closely follows Algorithm 6.4 in [Goodfellow-et-al-2016].</p> <pre>\nalgorithm: back-propagation\ninput: W<sup>(i)</sup>, i\u2208{1,...,l}; the weights of the model\n       b<sup>(i)</sup>, i\u2208{1,...,l}; the biases of the model\n       x, the input (features)\n       y, the output of the model\n       t, the targets\nrequire: f, activation function\n         f', derivative of the activation function\n\ng = \u2207<sub>y</sub>J (grad_y_loss)\nfor k=l,l-1,...,1 do\n   # \u2299 means element-wise multiplication NOT dot product\n   # This can be achieved by ordinary multiplication between two\n   # numpy 1-d arrays\n   g = g \u2299 f'(a<sup>(k)</sup>)\n   # Compute the gradients on the weight matrices and bias vectors\n   grad_bk_loss = g\n   # \u2297 means outer product, np.outer\n   grad_Wk_loss = h<sup>(k-1)</sup> \u2297 g\n   # Here W is a matrix and g is a vector so use np.matmul\n   # This step propagates the gradient to the layer below (k-1)\n   g = W<sup>(k)</sup>g\nend for\n</pre> <p>The method of the <code>Network</code> class to implement is <code>backprop</code> (see below). <pre><code>class Network:\n    ...\n\n    def backprop(self, targets: np.ndarray):\n        \"\"\"Implements the backpropagation algorithm\n        \"\"\" \n        assert self.output.shape == targets.shape\n        grad = (targets - self.output)\n        ...\n        return None\n</code></pre></p>"}]}